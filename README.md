# üï∏Ô∏è Web Scraping using BeautifulSoup or Selenium

When we cannot use an API key to collect data, we explore other ways to gather the information we need. One such method is web scraping, which involves extracting data from websites. However, before scraping, we must verify the website's "robots.txt" file to ensure that scraping is allowed and to follow the specified guidelines.

For example, here is the link to Wikipedia's "robots.txt" file:

- [Wikipedia robots.txt](https://en.wikipedia.org/robots.txt)

In this project, we demonstrate how to scrape a table from Wikipedia's "List of countries by GDP (nominal)" page using BeautifulSoup, a Python library for parsing HTML and XML documents. We also show how to scrape publication data from the DBLP computer science bibliography website using Selenium, a popular tool for automating web browsers.

Here are the links to the Jupyter notebooks for each part of the project:

- [Web Scrapping using BeautifulSoup](https://github.com/CatelloTheDataProjectManager/Web_Scrapping/blob/main/Web_Scrapping_using_BeautifulSoup.ipynb)
- [Web Scrapping using Selenium](https://github.com/CatelloTheDataProjectManager/Web_Scrapping/blob/main/Web_Scrapping_using_Selenium.ipynb)

By following along with the notebooks, you can learn how to extract data from websites, clean and format the data, and analyze it to gain insights. Web scraping is a valuable skill for data analysts, researchers, and anyone who needs to gather data from the web.
